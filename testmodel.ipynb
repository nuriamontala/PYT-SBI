{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "THINGS TO IMPORT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nx\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GATConv, global_add_pool\n",
    "from torch_geometric.data import Data\n",
    "import graphein.protein as gp\n",
    "from graphein.protein.config import ProteinGraphConfig\n",
    "from Bio import PDB\n",
    "import os\n",
    "from torch_geometric.data import Data, Dataset\n",
    "import pandas as pd\n",
    "from graphein.protein.config import ProteinGraphConfig\n",
    "from graphein.protein.graphs import construct_graph\n",
    "from graphein.protein.edges.distance import (add_peptide_bonds,\n",
    "                                             add_hydrogen_bond_interactions,\n",
    "                                             add_disulfide_interactions,\n",
    "                                             add_ionic_interactions,\n",
    "                                             add_aromatic_interactions,\n",
    "                                             add_aromatic_sulphur_interactions,\n",
    "                                             add_cation_pi_interactions,\n",
    "                                             add_delaunay_triangulation)\n",
    "\n",
    "# Edge construction functions\n",
    "new_edge_funcs = {\"edge_construction_functions\": [\n",
    "    add_peptide_bonds,\n",
    "    add_aromatic_interactions,\n",
    "    add_hydrogen_bond_interactions,\n",
    "    add_disulfide_interactions,\n",
    "    add_ionic_interactions,\n",
    "    add_aromatic_sulphur_interactions,\n",
    "    add_cation_pi_interactions,\n",
    "    add_delaunay_triangulation\n",
    "]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TESTING THE BUILDING OF THE GNN WITH GRAPH ATTENTION LAYERS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GET THE FEATURES FROM THE CSV FILE AND OBTAIN THE GRAPH OBJECT (literally copying from the proofmodel bindingfeatures.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f64fac3b66ee4216b9d40b189317e814",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìå Processing 3emh.pdb...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46081fb5f8f3447381b8da1cf3d9cbd6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Processed 3emh.pdb (306 nodes, 2173 edges)\n",
      "üìå Processing 1h28.pdb...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31b0a352a18f43fd9e63bcdcc1b352b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Processed 1h28.pdb (1126 nodes, 8540 edges)\n",
      "üìå Processing 2wik.pdb...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1bae28cf3d664e1d94b644319126d21b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Processed 2wik.pdb (527 nodes, 3881 edges)\n",
      "üìå Processing 1fig.pdb...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c2cfd5811f440e984a2854ea6e34324",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Processed 1fig.pdb (431 nodes, 3145 edges)\n",
      "üìå Processing 2mpa.pdb...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Processed 2mpa.pdb (450 nodes, 3295 edges)\n",
      "\n",
      "üîπ Total Processed Proteins: 5\n",
      "Graph 1: Data(x=[306, 71], edge_index=[2, 2173], y=[306])\n",
      "Graph 2: Data(x=[1126, 71], edge_index=[2, 8540], y=[1126])\n",
      "Graph 3: Data(x=[527, 71], edge_index=[2, 3881], y=[527])\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.data import Dataset\n",
    "\n",
    "class ProteinGraphDataset(Dataset):\n",
    "    \"\"\"Custom PyTorch Geometric Dataset to handle protein graphs.\"\"\"\n",
    "    def __init__(self, protein_graphs):\n",
    "        super().__init__()\n",
    "        self.protein_graphs = protein_graphs  # List of PyG Data objects\n",
    "\n",
    "    def len(self):\n",
    "        return len(self.protein_graphs)\n",
    "\n",
    "    def get(self, idx):\n",
    "        return self.protein_graphs[idx]\n",
    "\n",
    "def process_protein_graphs(folder_path):\n",
    "    \"\"\"\n",
    "    Processes all PDB files in a given folder, loads corresponding CSVs,\n",
    "    and returns a ProteinGraphDataset for PyTorch Geometric.\n",
    "    \"\"\"\n",
    "    protein_graphs = []\n",
    "    \n",
    "    # Get all PDB files in folder\n",
    "    pdb_files = [f for f in os.listdir(folder_path) if f.endswith(\".pdb\")]\n",
    "\n",
    "    for pdb_file in pdb_files:\n",
    "        pdb_path = os.path.join(folder_path, pdb_file)\n",
    "        csv_path = os.path.join(folder_path, pdb_file.replace(\".pdb\", \".csv\"))\n",
    "\n",
    "        if not os.path.exists(csv_path):\n",
    "            print(f\"‚ö†Ô∏è Warning: No CSV found for {pdb_file}. Skipping!\")\n",
    "            continue  # Skip this PDB if no matching CSV\n",
    "\n",
    "        print(f\"üìå Processing {pdb_file}...\")\n",
    "\n",
    "        # Load features & labels\n",
    "        features_dict, labels_dict, num_features = load_residue_features(csv_path)\n",
    "\n",
    "        # Construct NetworkX graph\n",
    "        config = ProteinGraphConfig(**new_edge_funcs)\n",
    "        G_nx = construct_graph(config=config, path=pdb_path)\n",
    "\n",
    "        # Convert to PyG Data object\n",
    "        protein_graph_data, reverse_map = networkx_to_pyg(G_nx, features_dict, labels_dict, num_features)\n",
    "\n",
    "        # Store the graph\n",
    "        protein_graphs.append(protein_graph_data)\n",
    "\n",
    "        print(f\"‚úÖ Processed {pdb_file} ({len(G_nx.nodes())} nodes, {len(G_nx.edges())} edges)\")\n",
    "\n",
    "    # ‚úÖ Return as a PyTorch Geometric Dataset\n",
    "    return ProteinGraphDataset(protein_graphs)\n",
    "\n",
    "# Folder containing PDB & CSV files\n",
    "folder_path = \"testingfiles\"  # CHANGE THIS!\n",
    "\n",
    "# ‚úÖ Process all proteins into a PyG Dataset\n",
    "protein_graph_dataset = process_protein_graphs(folder_path)\n",
    "\n",
    "# ‚úÖ Print dataset summary\n",
    "print(f\"\\nüîπ Total Processed Proteins: {len(protein_graph_dataset)}\")\n",
    "for i, graph in enumerate(protein_graph_dataset[:3]):  # Show first 3 graphs\n",
    "    print(f\"Graph {i+1}: {graph}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DATA LOADER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/adam-marti/anaconda3/envs/MSI_HO/lib/python3.12/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.data import DataLoader\n",
    "\n",
    "# Split dataset into train/test sets\n",
    "train_size = int(0.8 * len(protein_graph_dataset))\n",
    "test_size = len(protein_graph_dataset) - train_size\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(protein_graph_dataset, [train_size, test_size])\n",
    "\n",
    "# ‚úÖ Create DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NEURAL NETWORK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30 - Loss: 0.4721, Test Accuracy: 0.8933\n",
      "Epoch 2/30 - Loss: 0.3478, Test Accuracy: 0.8933\n",
      "Epoch 3/30 - Loss: 0.2969, Test Accuracy: 0.8933\n",
      "Epoch 4/30 - Loss: 0.2919, Test Accuracy: 0.8933\n",
      "Epoch 5/30 - Loss: 0.3056, Test Accuracy: 0.8933\n",
      "Epoch 6/30 - Loss: 0.3203, Test Accuracy: 0.8933\n",
      "Epoch 7/30 - Loss: 0.3134, Test Accuracy: 0.8933\n",
      "Epoch 8/30 - Loss: 0.3202, Test Accuracy: 0.8933\n",
      "Epoch 9/30 - Loss: 0.2990, Test Accuracy: 0.8933\n",
      "Epoch 10/30 - Loss: 0.2969, Test Accuracy: 0.8933\n",
      "Epoch 11/30 - Loss: 0.2786, Test Accuracy: 0.8933\n",
      "Epoch 12/30 - Loss: 0.2790, Test Accuracy: 0.8933\n",
      "Epoch 13/30 - Loss: 0.2796, Test Accuracy: 0.8933\n",
      "Epoch 14/30 - Loss: 0.2720, Test Accuracy: 0.8933\n",
      "Epoch 15/30 - Loss: 0.2787, Test Accuracy: 0.8933\n",
      "Epoch 16/30 - Loss: 0.2805, Test Accuracy: 0.8933\n",
      "Epoch 17/30 - Loss: 0.2835, Test Accuracy: 0.8933\n",
      "Epoch 18/30 - Loss: 0.2811, Test Accuracy: 0.8933\n",
      "Epoch 19/30 - Loss: 0.2816, Test Accuracy: 0.8933\n",
      "Epoch 20/30 - Loss: 0.2773, Test Accuracy: 0.8933\n",
      "Epoch 21/30 - Loss: 0.2719, Test Accuracy: 0.8933\n",
      "Epoch 22/30 - Loss: 0.2757, Test Accuracy: 0.8933\n",
      "Epoch 23/30 - Loss: 0.2737, Test Accuracy: 0.8933\n",
      "Epoch 24/30 - Loss: 0.2668, Test Accuracy: 0.8933\n",
      "Epoch 25/30 - Loss: 0.2680, Test Accuracy: 0.8933\n",
      "Epoch 26/30 - Loss: 0.2728, Test Accuracy: 0.8933\n",
      "Epoch 27/30 - Loss: 0.2727, Test Accuracy: 0.8933\n",
      "Epoch 28/30 - Loss: 0.2674, Test Accuracy: 0.8933\n",
      "Epoch 29/30 - Loss: 0.2710, Test Accuracy: 0.8933\n",
      "Epoch 30/30 - Loss: 0.2710, Test Accuracy: 0.8933\n",
      "‚úÖ Training Completed!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch_geometric.data import Data, Dataset, DataLoader\n",
    "from torch_geometric.nn import GATConv, global_mean_pool\n",
    "\n",
    "# ‚úÖ Extend Dataset class to handle protein graphs\n",
    "class ProteinGraphDataset(Dataset):\n",
    "    def __init__(self, protein_graphs):\n",
    "        super().__init__()\n",
    "        self.protein_graphs = protein_graphs  # List of Data objects\n",
    "\n",
    "    def len(self):\n",
    "        return len(self.protein_graphs)\n",
    "\n",
    "    def get(self, idx):\n",
    "        return self.protein_graphs[idx]\n",
    "\n",
    "# ‚úÖ Create Virtual Node GAT Model\n",
    "class GATVirtualNode(nn.Module):\n",
    "    def __init__(self, in_features, hidden_dim, num_heads, num_classes, dropout=0.2):\n",
    "        super(GATVirtualNode, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_heads = num_heads\n",
    "\n",
    "        # Virtual Node (Global Graph Representation)\n",
    "        self.virtual_node_embedding = nn.Parameter(torch.zeros(1, hidden_dim))\n",
    "        \n",
    "        # Graph Attention Layers\n",
    "        self.gat1 = GATConv(in_features, hidden_dim // num_heads, heads=num_heads)\n",
    "        self.gat2 = GATConv(hidden_dim, hidden_dim // num_heads, heads=num_heads)\n",
    "\n",
    "        # MLP Classifier for **node classification**\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim // 2, num_classes)  # Output for **each node**\n",
    "        )\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "\n",
    "        # Apply first GAT layer\n",
    "        x = self.gat1(x, edge_index)\n",
    "        x = torch.relu(x)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        # Apply second GAT layer\n",
    "        x = self.gat2(x, edge_index)\n",
    "        x = torch.relu(x)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        # Virtual Node Aggregation\n",
    "        virtual_node = self.virtual_node_embedding.expand(x.size(0), -1)\n",
    "        x = x + virtual_node\n",
    "\n",
    "        # **Return node-level predictions**\n",
    "        return self.mlp(x)  # Output has same shape as `data.y`\n",
    "\n",
    "\n",
    "# ‚úÖ Train the model\n",
    "def train(model, train_loader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for data in train_loader:\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        out = model(data)  # **Now out has shape (num_nodes, num_classes)**\n",
    "        loss = criterion(out, data.y)  # **Matches (num_nodes, num_classes) vs (num_nodes,)**\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    return total_loss / len(train_loader)\n",
    "\n",
    "\n",
    "# ‚úÖ Evaluate the model\n",
    "def evaluate(model, test_loader, device):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data in test_loader:\n",
    "            data = data.to(device)\n",
    "            out = model(data)  # Shape: (num_nodes, num_classes)\n",
    "            pred = out.argmax(dim=1)  # Get class with highest probability\n",
    "\n",
    "            correct += (pred == data.y).sum().item()  # Compare node-wise\n",
    "            total += data.y.size(0)\n",
    "\n",
    "    return correct / total  # Node-level accuracy\n",
    "\n",
    "\n",
    "# ‚úÖ Load Protein Dataset\n",
    "protein_graph_dataset = ProteinGraphDataset(protein_graph_dataset)  # Wrap list into PyG Dataset\n",
    "\n",
    "# ‚úÖ Split dataset into train/test\n",
    "train_size = int(0.8 * len(protein_graph_dataset))\n",
    "test_size = len(protein_graph_dataset) - train_size\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(protein_graph_dataset, [train_size, test_size])\n",
    "\n",
    "# ‚úÖ Create DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=4, shuffle=False)\n",
    "\n",
    "# ‚úÖ Define Model, Loss, Optimizer\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = GATVirtualNode(in_features=protein_graph_dataset[0].x.shape[1], hidden_dim=128, num_heads=4, num_classes=2).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-4)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# ‚úÖ Training Loop\n",
    "num_epochs = 30\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = train(model, train_loader, optimizer, criterion, device)\n",
    "    test_acc = evaluate(model, test_loader, device)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} - Loss: {train_loss:.4f}, Test Accuracy: {test_acc:.4f}\")\n",
    "\n",
    "print(\"‚úÖ Training Completed!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FULL CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "570776ba792f48129c385855735c12bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìå Processing 3emh.pdb...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45f7be02644b4e569ee5c881a5f53dd0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Processed 3emh.pdb (306 nodes, 2173 edges)\n",
      "üìå Processing 1h28.pdb...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "976e9196ece44c6aadcb7984751772d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Processed 1h28.pdb (1126 nodes, 8540 edges)\n",
      "üìå Processing 2wik.pdb...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1038dbc488804154a70e6b1b98304187",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Processed 2wik.pdb (527 nodes, 3881 edges)\n",
      "üìå Processing 1fig.pdb...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0dfe715deb646e883615da96c5423d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Processed 1fig.pdb (431 nodes, 3145 edges)\n",
      "üìå Processing 2mpa.pdb...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Processed 2mpa.pdb (450 nodes, 3295 edges)\n",
      "Epoch 1/30 - Loss: 1.0321, Test Accuracy: 0.8933\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/adam-marti/anaconda3/envs/MSI_HO/lib/python3.12/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/30 - Loss: 0.5381, Test Accuracy: 0.8933\n",
      "Epoch 3/30 - Loss: 0.3500, Test Accuracy: 0.8933\n",
      "Epoch 4/30 - Loss: 0.3196, Test Accuracy: 0.8933\n",
      "Epoch 5/30 - Loss: 0.3284, Test Accuracy: 0.8933\n",
      "Epoch 6/30 - Loss: 0.3524, Test Accuracy: 0.8933\n",
      "Epoch 7/30 - Loss: 0.3709, Test Accuracy: 0.8933\n",
      "Epoch 8/30 - Loss: 0.3838, Test Accuracy: 0.8933\n",
      "Epoch 9/30 - Loss: 0.3805, Test Accuracy: 0.8933\n",
      "Epoch 10/30 - Loss: 0.3700, Test Accuracy: 0.8933\n",
      "Epoch 11/30 - Loss: 0.3540, Test Accuracy: 0.8933\n",
      "Epoch 12/30 - Loss: 0.3375, Test Accuracy: 0.8933\n",
      "Epoch 13/30 - Loss: 0.3255, Test Accuracy: 0.8933\n",
      "Epoch 14/30 - Loss: 0.3152, Test Accuracy: 0.8933\n",
      "Epoch 15/30 - Loss: 0.2957, Test Accuracy: 0.8933\n",
      "Epoch 16/30 - Loss: 0.3010, Test Accuracy: 0.8933\n",
      "Epoch 17/30 - Loss: 0.2905, Test Accuracy: 0.8933\n",
      "Epoch 18/30 - Loss: 0.2894, Test Accuracy: 0.8933\n",
      "Epoch 19/30 - Loss: 0.2873, Test Accuracy: 0.8933\n",
      "Epoch 20/30 - Loss: 0.2919, Test Accuracy: 0.8933\n",
      "Epoch 21/30 - Loss: 0.2953, Test Accuracy: 0.8933\n",
      "Epoch 22/30 - Loss: 0.2930, Test Accuracy: 0.8933\n",
      "Epoch 23/30 - Loss: 0.2896, Test Accuracy: 0.8933\n",
      "Epoch 24/30 - Loss: 0.2896, Test Accuracy: 0.8933\n",
      "Epoch 25/30 - Loss: 0.2840, Test Accuracy: 0.8933\n",
      "Epoch 26/30 - Loss: 0.2801, Test Accuracy: 0.8933\n",
      "Epoch 27/30 - Loss: 0.2739, Test Accuracy: 0.8933\n",
      "Epoch 28/30 - Loss: 0.2718, Test Accuracy: 0.8933\n",
      "Epoch 29/30 - Loss: 0.2703, Test Accuracy: 0.8933\n",
      "Epoch 30/30 - Loss: 0.2722, Test Accuracy: 0.8933\n",
      "‚úÖ Training Completed!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "from torch_geometric.data import Data, Dataset, DataLoader\n",
    "from torch_geometric.nn import GATConv, global_mean_pool\n",
    "from graphein.protein.config import ProteinGraphConfig\n",
    "from graphein.protein.graphs import construct_graph\n",
    "from graphein.protein.edges.distance import (add_peptide_bonds,\n",
    "                                             add_hydrogen_bond_interactions,\n",
    "                                             add_disulfide_interactions,\n",
    "                                             add_ionic_interactions,\n",
    "                                             add_aromatic_interactions,\n",
    "                                             add_aromatic_sulphur_interactions,\n",
    "                                             add_cation_pi_interactions,\n",
    "                                             add_delaunay_triangulation)\n",
    "\n",
    "# ‚úÖ Define Edge Functions\n",
    "new_edge_funcs = {\"edge_construction_functions\": [add_peptide_bonds,\n",
    "                                                  add_aromatic_interactions,\n",
    "                                                  add_hydrogen_bond_interactions,\n",
    "                                                  add_disulfide_interactions,\n",
    "                                                  add_ionic_interactions,\n",
    "                                                  add_aromatic_sulphur_interactions,\n",
    "                                                  add_cation_pi_interactions,\n",
    "                                                  add_delaunay_triangulation]}\n",
    "\n",
    "# ‚úÖ Load Residue Features from CSV\n",
    "def load_residue_features(csv_path):\n",
    "    \"\"\"Loads residue features and binding site labels from CSV.\"\"\"\n",
    "    df = pd.read_csv(csv_path)\n",
    "\n",
    "    # Convert first column (Residue ID) into tuples (residue_number, chain)\n",
    "    df.iloc[:, 0] = df.iloc[:, 0].apply(eval)  # Convert string \"(1, 'L')\" ‚Üí tuple (1, 'L')\n",
    "\n",
    "    residue_ids = df.iloc[:, 0]  # Residue ID as tuple\n",
    "    features = df.iloc[:, 1:-1].values  # Feature columns (excluding label)\n",
    "    labels = df.iloc[:, -1].values  # Last column = binding site labels\n",
    "\n",
    "    # Create lookup dictionaries\n",
    "    features_dict = {res_id: feat for res_id, feat in zip(residue_ids, features)}\n",
    "    labels_dict = {res_id: label for res_id, label in zip(residue_ids, labels)}\n",
    "\n",
    "    return features_dict, labels_dict, features.shape[1]  # Return num_features\n",
    "\n",
    "# ‚úÖ Convert NetworkX Graph ‚Üí PyTorch Geometric Graph\n",
    "def networkx_to_pyg(G_nx, features_dict, labels_dict, num_features):\n",
    "    \"\"\"Converts a NetworkX protein graph to a PyTorch Geometric Data object with features & labels.\"\"\"\n",
    "    node_map = {}  # Maps node (residue, chain) to index\n",
    "    reverse_map = {}  # Reverse lookup: PyG index ‚Üí (residue_number, chain)\n",
    "    node_features = []\n",
    "    y = []\n",
    "\n",
    "    for i, (node, attr) in enumerate(G_nx.nodes(data=True)):\n",
    "        res_id = (attr.get(\"residue_number\"), attr.get(\"chain_id\"))  # Standardized format\n",
    "\n",
    "        node_map[node] = i  # Assign PyG-compatible node index\n",
    "        reverse_map[i] = res_id  # Store mapping back to residue identifier\n",
    "\n",
    "        # Retrieve features (default to zero vector if missing)\n",
    "        features = features_dict.get(res_id, [0] * num_features)\n",
    "        node_features.append(features)\n",
    "\n",
    "        # Retrieve binding site labels (default = non-binding)\n",
    "        y.append(labels_dict.get(res_id, 0))\n",
    "\n",
    "    x = torch.tensor(node_features, dtype=torch.float)\n",
    "    y = torch.tensor(y, dtype=torch.long)\n",
    "\n",
    "    # Extract edges\n",
    "    edges = []\n",
    "    for u, v in G_nx.edges():\n",
    "        try:\n",
    "            edges.append((node_map[u], node_map[v]))  # Use fixed node IDs\n",
    "        except KeyError:\n",
    "            print(f\"‚ö†Ô∏è Skipping edge ({u}, {v}) due to missing node mapping!\")\n",
    "\n",
    "    edge_index = torch.tensor(edges, dtype=torch.long).t().contiguous()\n",
    "\n",
    "    return Data(x=x, edge_index=edge_index, y=y), reverse_map\n",
    "\n",
    "# ‚úÖ Process All PDB + CSV Files in a Folder\n",
    "def process_protein_graphs(folder_path):\n",
    "    \"\"\"Processes all PDB files in a given folder & returns a ProteinGraphDataset.\"\"\"\n",
    "    protein_graphs = []\n",
    "    \n",
    "    # Get all PDB files in folder\n",
    "    pdb_files = [f for f in os.listdir(folder_path) if f.endswith(\".pdb\")]\n",
    "\n",
    "    for pdb_file in pdb_files:\n",
    "        pdb_path = os.path.join(folder_path, pdb_file)\n",
    "        csv_path = os.path.join(folder_path, pdb_file.replace(\".pdb\", \".csv\"))\n",
    "\n",
    "        if not os.path.exists(csv_path):\n",
    "            print(f\"‚ö†Ô∏è Warning: No CSV found for {pdb_file}. Skipping!\")\n",
    "            continue  # Skip this PDB if no matching CSV\n",
    "\n",
    "        print(f\"üìå Processing {pdb_file}...\")\n",
    "\n",
    "        # Load features & labels\n",
    "        features_dict, labels_dict, num_features = load_residue_features(csv_path)\n",
    "\n",
    "        # Construct NetworkX graph\n",
    "        config = ProteinGraphConfig(**new_edge_funcs)\n",
    "        G_nx = construct_graph(config=config, path=pdb_path)\n",
    "\n",
    "        # Convert to PyG Data object\n",
    "        protein_graph_data, reverse_map = networkx_to_pyg(G_nx, features_dict, labels_dict, num_features)\n",
    "\n",
    "        # Store the graph\n",
    "        protein_graphs.append(protein_graph_data)\n",
    "\n",
    "        print(f\"‚úÖ Processed {pdb_file} ({len(G_nx.nodes())} nodes, {len(G_nx.edges())} edges)\")\n",
    "\n",
    "    return ProteinGraphDataset(protein_graphs)\n",
    "\n",
    "# ‚úÖ Define PyTorch Geometric Dataset\n",
    "class ProteinGraphDataset(Dataset):\n",
    "    \"\"\"Custom PyTorch Geometric Dataset to handle protein graphs.\"\"\"\n",
    "    def __init__(self, protein_graphs):\n",
    "        super().__init__()\n",
    "        self.protein_graphs = protein_graphs  # List of Data objects\n",
    "\n",
    "    def len(self):\n",
    "        return len(self.protein_graphs)\n",
    "\n",
    "    def get(self, idx):\n",
    "        return self.protein_graphs[idx]\n",
    "\n",
    "# ‚úÖ Define Virtual Node GAT Model\n",
    "class GATVirtualNode(nn.Module):\n",
    "    def __init__(self, in_features, hidden_dim, num_heads, num_classes, dropout=0.2):\n",
    "        super(GATVirtualNode, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_heads = num_heads\n",
    "\n",
    "        # Virtual Node (Global Graph Representation)\n",
    "        self.virtual_node_embedding = nn.Parameter(torch.zeros(1, hidden_dim))\n",
    "        \n",
    "        # Graph Attention Layers\n",
    "        self.gat1 = GATConv(in_features, hidden_dim // num_heads, heads=num_heads)\n",
    "        self.gat2 = GATConv(hidden_dim, hidden_dim // num_heads, heads=num_heads)\n",
    "\n",
    "        # MLP Classifier for **node classification**\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim // 2, num_classes)  # Output for **each node**\n",
    "        )\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "\n",
    "        # Apply first GAT layer\n",
    "        x = self.gat1(x, edge_index)\n",
    "        x = torch.relu(x)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        # Apply second GAT layer\n",
    "        x = self.gat2(x, edge_index)\n",
    "        x = torch.relu(x)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        # Virtual Node Aggregation\n",
    "        virtual_node = self.virtual_node_embedding.expand(x.size(0), -1)\n",
    "        x = x + virtual_node\n",
    "\n",
    "        # **Return node-level predictions**\n",
    "        return self.mlp(x)  # Output has same shape as `data.y`\n",
    "\n",
    "\n",
    "\n",
    "# ‚úÖ Load & Prepare Dataset\n",
    "folder_path = \"testingfiles\"  # CHANGE THIS!\n",
    "protein_graph_dataset = process_protein_graphs(folder_path)\n",
    "\n",
    "# ‚úÖ Split dataset into train/test\n",
    "train_size = int(0.8 * len(protein_graph_dataset))\n",
    "test_size = len(protein_graph_dataset) - train_size\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(protein_graph_dataset, [train_size, test_size])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=4, shuffle=False)\n",
    "\n",
    "# ‚úÖ Define Model, Loss, Optimizer\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = GATVirtualNode(in_features=protein_graph_dataset[0].x.shape[1], hidden_dim=128, num_heads=4, num_classes=2).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-4)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# ‚úÖ Training Loop\n",
    "num_epochs = 30\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = train(model, train_loader, optimizer, criterion, device)\n",
    "    test_acc = evaluate(model, test_loader, device)\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} - Loss: {train_loss:.4f}, Test Accuracy: {test_acc:.4f}\")\n",
    "\n",
    "print(\"‚úÖ Training Completed!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TESTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MSI_HO",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
